{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_4o9jvsh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E5E7AD87C95F4C6E844942E1958748A5","mdEditEnable":false},"source":"# Deep Convolutional Generative Adversarial Networks\n\nwe introduced the basic ideas behind how GANs work. We showed that they can draw samples from some simple, easy-to-sample distribution, like a uniform or normal distribution, and transform them into samples that appear to match the distribution of some dataset. And while our example of matching a 2D Gaussian distribution got the point across, it is not especially exciting.\n\nIn this section, we will demonstrate how you can use GANs to generate photorealistic images. We will be basing our models on the deep convolutional GANs (DCGAN) introduced in :cite:`Radford.Metz.Chintala.2015`. We will borrow the convolutional architecture that have proven so successful for discriminative computer vision problems and show how via GANs, they can be leveraged to generate photorealistic images."},{"cell_type":"code","execution_count":1,"metadata":{"attributes":{"classes":[],"id":"","n":"1"},"graffitiCellId":"id_cvcd9mu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EF7985D16EF74FC99C139730F46B293D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"True\n","name":"stdout"}],"source":"import matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nimport zipfile\ncuda = True if torch.cuda.is_available() else False\nprint(cuda)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_n7p7u54","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EEBE49404E624C5BB52CFFF630D34375","mdEditEnable":false},"source":"## The Pokemon Dataset\n\nThe dataset we will use is a collection of Pokemon sprites obtained from [pokemondb](https://pokemondb.net/sprites). First download, extract and load this dataset."},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_6auh67d","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2F378B9E5C754D64843F7E42B467B245","mdEditEnable":false},"source":"We resize each image into $64\\times 64$. The `ToTensor` transformation will project the pixel value into $[0, 1]$, while our generator will use the tanh function to obtain outputs in $[-1, 1]$. Therefore we normalize the data with $0.5$ mean and $0.5$ standard deviation to match the value range."},{"cell_type":"code","execution_count":7,"metadata":{"attributes":{"classes":[],"id":"","n":"3"},"graffitiCellId":"id_d8f9dcp","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DA24565845CA418CB718F40ED99347C8","collapsed":false,"scrolled":false},"outputs":[],"source":"data_dir='/home/kesci/input/pokemon8600/'\nbatch_size=256\ntransform=transforms.Compose([\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),  #[0,1]之间的数字\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])\npokemon=ImageFolder(data_dir+'pokemon',transform)\ndata_iter=DataLoader(pokemon,batch_size=batch_size,shuffle=True)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ysau10i","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C2E5276764E84908980E17A01E82BB0E","mdEditEnable":false},"source":"Let's visualize the first 20 images."},{"metadata":{"id":"BBF1211DBA69478C83F2A137B1E17335","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"Dataset ImageFolder\n    Number of datapoints: 40597\n    Root location: /home/kesci/input/pokemon8600/pokemon"},"transient":{},"execution_count":8}],"source":"data_iter.dataset","execution_count":8},{"cell_type":"code","execution_count":9,"metadata":{"attributes":{"classes":[],"id":"","n":"4"},"graffitiCellId":"id_eir3a70","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0106E502D1D840CF8557FCDFFB22742A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 360x360 with 20 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/0106E502D1D840CF8557FCDFFB22742A/q66o13s9b8.png\">"},"transient":{}}],"source":"fig=plt.figure(figsize=(5,5))\nimgs=data_iter.dataset.imgs\nfor i in range(20):\n    img = plt.imread(imgs[i*150][0])\n    plt.subplot(4,5,i+1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_w4wzsea","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5C28D43EE05D43738CD229AD028B6F0F","mdEditEnable":false},"source":"## The Generator\n\nThe generator needs to map the noise variable $\\mathbf z\\in\\mathbb R^d$, a length-$d$ vector, to a RGB image with width and height to be $64\\times 64$ . In :numref:`sec_fcn` we introduced the fully convolutional network that uses transposed convolution layer (refer to :numref:`sec_transposed_conv`) to enlarge input size. The basic block of the generator contains a transposed convolution layer followed by the batch normalization and ReLU activation."},{"cell_type":"code","execution_count":10,"metadata":{"attributes":{"classes":[],"id":"","n":"5"},"graffitiCellId":"id_kcbnh1m","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"13A8524A3D784D66AF23F658D854806A","collapsed":false,"scrolled":false},"outputs":[],"source":"# 将噪声变量转化成图像的大小\nclass G_block(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=4,strides=2, padding=1):\n        super(G_block,self).__init__()\n        #转置卷积层\n        self.conv2d_trans=nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n                                             stride=strides, padding=padding, bias=False) \n        self.batch_norm=nn.BatchNorm2d(out_channels,0.8)\n        self.activation=nn.ReLU()\n    def forward(self,x):\n        return self.activation(self.batch_norm(self.conv2d_trans(x)))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_t731jok","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B62B15D5AA954BA896FBD8C61F40D0D3","mdEditEnable":false},"source":"In default, the transposed convolution layer uses a $k_h = k_w = 4$ kernel, a $s_h = s_w = 2$ strides, and a $p_h = p_w = 1$ padding. With a input shape of $n_h^{'} \\times n_w^{'} = 16 \\times 16$, the generator block will double input's width and height.\n\n\n$$\n\\begin{aligned}\nn_h^{'} \\times n_w^{'} &= [(n_h k_h - (n_h-1)(k_h-s_h)- 2p_h] \\times [(n_w k_w - (n_w-1)(k_w-s_w)- 2p_w]\\\\\n  &= [(k_h + s_h (n_h-1)- 2p_h] \\times [(k_w + s_w (n_w-1)- 2p_w]\\\\\n  &= [(4 + 2 \\times (16-1)- 2 \\times 1] \\times [(4 + 2 \\times (16-1)- 2 \\times 1]\\\\\n  &= 32 \\times 32 .\\\\\n\\end{aligned}\n$$\n"},{"cell_type":"code","execution_count":11,"metadata":{"attributes":{"classes":[],"id":"","n":"6"},"graffitiCellId":"id_on8avly","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"42A9BB159B494CE49E2E843B61D18D3F","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"torch.Size([2, 20, 32, 32])\n","name":"stdout"}],"source":"Tensor=torch.cuda.FloatTensor\nx=Variable(Tensor(np.zeros((2,3,16,16))))\ng_blk=G_block(3,20)\ng_blk.cuda()\nprint(g_blk(x).shape)"},{"metadata":{"id":"B241EC8A120744DD8E226E60E4D0FE63","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n\nin_channels (python:int) – Number of channels in the input image\n\nout_channels (python:int) – Number of channels produced by the convolution\n\nkernel_size (python:int or tuple) – Size of the convolving kernel\n\nstride (python:int or tuple, optional) – Stride of the convolution. Default: 1\n\npadding (python:int or tuple, optional) – dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0\n\noutput_padding (python:int or tuple, optional) – Additional size added to one side of each dimension in the output shape. Default: 0\n\ngroups (python:int, optional) – Number of blocked connections from input channels to output channels. Default: 1\n\nbias (bool, optional) – If True, adds a learnable bias to the output. Default: True\n\ndilation (python:int or tuple, optional) – Spacing between kernel elements. Default: 1\n\nH_out =(H_in−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\n \n反卷积：\noutput = (input-1)*stridee+outputpaddng -2*padding + kernelsize\nout = (in-1)*stride -2*padding + dilation*(kernel_size-1)+output_padding +1\n(16-1)*2-2*1+1*(4-1)+0+1\n"},{"metadata":{"id":"6CB180A7ECC545FF851E75A360A326D8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"32"},"transient":{},"execution_count":12}],"source":"(16-1)*2-2*1+1*(4-1)+0+1\n","execution_count":12},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_2o68o2c","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BE982C2C1F0B4C678C1832271FF89010","mdEditEnable":false},"source":"If changing the transposed convolution layer to a $4\\times 4$ kernel, $1\\times 1$ strides and zero padding. With a input size of $1 \\times 1$, the output will have its width and height increased by 3 respectively."},{"cell_type":"code","execution_count":13,"metadata":{"attributes":{"classes":[],"id":"","n":"7"},"graffitiCellId":"id_ohaef4t","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8F915B91477F45F197CC00A520BDD33B","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"torch.Size([2, 20, 4, 4])\n","name":"stdout"}],"source":"x=Variable(Tensor(np.zeros((2,3,1,1))))\ng_blk=G_block(3,20,strides=1,padding=0)\ng_blk.cuda()\nprint(g_blk(x).shape)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_coxkw3t","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E7A9540D2F2B4959B62BA1A1117D558A","mdEditEnable":false},"source":"The generator consists of four basic blocks that increase input's both width and height from 1 to 32. At the same time, it first projects the latent variable into $64\\times 8$ channels, and then halve the channels each time. At last, a transposed convolution layer is used to generate the output. It further doubles the width and height to match the desired $64\\times 64$ shape, and reduces the channel size to $3$. The tanh activation function is applied to project output values into the $(-1, 1)$ range."},{"cell_type":"code","execution_count":14,"metadata":{"attributes":{"classes":[],"id":"","n":"8"},"graffitiCellId":"id_ol5nbva","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"93A7C73CB7B848B686DB37C5A4E8DF1B","collapsed":false,"scrolled":false},"outputs":[],"source":"class net_G(nn.Module):\n    def __init__(self,in_channels):\n        super(net_G,self).__init__()\n\n        n_G=64\n        self.model=nn.Sequential(\n            G_block(in_channels,n_G*8,strides=1,padding=0),\n            G_block(n_G*8,n_G*4),\n            G_block(n_G*4,n_G*2),\n            G_block(n_G*2,n_G),\n            nn.ConvTranspose2d(\n                n_G,3,kernel_size=4,stride=2,padding=1,bias=False\n            ),\n            nn.Tanh()\n        )\n    def forward(self,x):\n        x=self.model(x)\n        return x\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, mean=0, std=0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, mean=1.0, std=0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ut6tdcd","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D86B501F61AA4712A840F72E5F12BBAF","mdEditEnable":false},"source":"Generate a 100 dimensional latent variable to verify the generator's output shape."},{"cell_type":"code","execution_count":15,"metadata":{"attributes":{"classes":[],"id":"","n":"9"},"graffitiCellId":"id_z51zod2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D94838D3C7E84FD3A4B8CDEF6FFFD23B","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"torch.Size([1, 3, 64, 64])\n","name":"stdout"}],"source":"x=Variable(Tensor(np.zeros((1,100,1,1))))\ngenerator=net_G(100)\ngenerator.cuda()\ngenerator.apply(weights_init_normal)\nprint(generator(x).shape)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_i42g9u2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0BE3A58F5A6A4C4F92BEECD84B64C10B","mdEditEnable":false},"source":"## Discriminator\n\nThe discriminator is a normal convolutional network network except that it uses a leaky ReLU as its activation function. Given $\\alpha \\in[0, 1]$, its definition is\n\n\n$$\n\\textrm{leaky ReLU}(x) = \\begin{cases}x & \\text{if}\\ x > 0\\\\ \\alpha x &\\text{otherwise}\\end{cases}.\n$$\n\n\nAs it can be seen, it is normal ReLU if $\\alpha=0$, and an identity function if $\\alpha=1$. For $\\alpha \\in (0, 1)$, leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the \"dying ReLU\" problem that a neuron might always output a negative value and therefore cannot make any progress since the gradient of ReLU is 0."},{"cell_type":"code","execution_count":16,"metadata":{"attributes":{"classes":[],"id":"","n":"10"},"graffitiCellId":"id_4tobxs3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9AA265C8E2D54CB7851745C8BB41C84B","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 288x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9AA265C8E2D54CB7851745C8BB41C84B/q66pvyw34k.png\">"},"transient":{}}],"source":"alphas = [0, 0.2, 0.4, .6]\nx = np.arange(-2, 1, 0.1)\nY = [nn.LeakyReLU(alpha)(Tensor(x)).cpu().numpy()for alpha in alphas]\nplt.figure(figsize=(4,4))\nfor y in Y:\n    plt.plot(x,y)\nplt.show()"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_rua557b","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CC773AE0204644A28D8F3E33D57526FA","mdEditEnable":false},"source":"The basic block of the discriminator is a convolution layer followed by a batch normalization layer and a leaky ReLU activation. The hyper-parameters of the convolution layer are similar to the transpose convolution layer in the generator block."},{"cell_type":"code","execution_count":17,"metadata":{"attributes":{"classes":[],"id":"","n":"11"},"graffitiCellId":"id_sg2bxuv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F3165E3CEE2A415F8A3858B0E68F43C6","collapsed":false,"scrolled":false},"outputs":[],"source":"class D_block(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=4,strides=2,\n                 padding=1,alpha=0.2):\n        super(D_block,self).__init__()\n        self.conv2d=nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding,bias=False)\n        self.batch_norm=nn.BatchNorm2d(out_channels,0.8)\n        self.activation=nn.LeakyReLU(alpha)\n    def forward(self,X):\n        return self.activation(self.batch_norm(self.conv2d(X)))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_93f29b5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AEF7893313BA46998E2DE2193AE15E15","mdEditEnable":false},"source":"A basic block with default settings will halve the width and height of the inputs, as we demonstrated in :numref:`sec_padding`. For example, given a input shape $n_h = n_w = 16 $, with a kernel shape $k_h = k_w = 4$, a stride shape $s_h = s_w = 2$, and a padding shape $p_h = p_w = 1$, the output shape will be:\n\n\n$$\n\n\\begin{aligned}\nn_h^{'} \\times n_w^{'} &= \\lfloor(n_h-k_h+2p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+2p_w+s_w)/s_w\\rfloor\\\\\n  &= \\lfloor(16-4+2\\times 1+2)/2\\rfloor \\times \\lfloor(16-4+2\\times 1+2)/2\\rfloor\\\\\n  &= 8 \\times 8 .\\\\\n\\end{aligned}\n\n$$\n"},{"cell_type":"code","execution_count":18,"metadata":{"attributes":{"classes":[],"id":"","n":"12"},"graffitiCellId":"id_hi27kkc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3FC84D35530C4863AC53C2AA82414228","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"torch.Size([2, 20, 8, 8])\n","name":"stdout"}],"source":"x = Variable(Tensor(np.zeros((2, 3, 16, 16))))\nd_blk = D_block(3,20)\nd_blk.cuda()\nprint(d_blk(x).shape)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_gh8rm9u","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B8FF92411A3A4BF3B528713D754438C6","mdEditEnable":false},"source":"The discriminator is a mirror of the generator."},{"cell_type":"code","execution_count":19,"metadata":{"attributes":{"classes":[],"id":"","n":"13"},"graffitiCellId":"id_ilwjbsk","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BC124154CD234BDDAF91592CA31C514E","collapsed":false,"scrolled":false},"outputs":[],"source":"class net_D(nn.Module):\n    def __init__(self,in_channels):\n        super(net_D,self).__init__()\n        n_D=64\n        self.model=nn.Sequential(\n            D_block(in_channels,n_D),\n            D_block(n_D,n_D*2),\n            D_block(n_D*2,n_D*4),\n            D_block(n_D*4,n_D*8)\n        )\n        self.conv=nn.Conv2d(n_D*8,1,kernel_size=4,bias=False)\n        self.activation=nn.Sigmoid()\n        # self._initialize_weights()\n    def forward(self,x):\n        x=self.model(x)\n        x=self.conv(x)\n        x=self.activation(x)\n        return x"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3mhtj2z","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"71310D73F4544ADD8D37681AE7667A81","mdEditEnable":false},"source":"It uses a convolution layer with output channel $1$ as the last layer to obtain a single prediction value."},{"cell_type":"code","execution_count":20,"metadata":{"attributes":{"classes":[],"id":"","n":"15"},"graffitiCellId":"id_uurforl","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7C74B6FF8DBE42DDA521E38429DA77D4","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"torch.Size([1, 1, 1, 1])\n","name":"stdout"}],"source":"x = Variable(Tensor(np.zeros((1, 3, 64, 64))))\ndiscriminator=net_D(3)\ndiscriminator.cuda()\ndiscriminator.apply(weights_init_normal)\nprint(discriminator(x).shape)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_v5ytu3j","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FC1CD12966AA4B8D8F7AAA803EA8F20E","mdEditEnable":false},"source":"## Training\nCompared to the basic GAN in :numref:`sec_basic_gan`, we use the same learning rate for both generator and discriminator since they are similar to each other. In addition, we change $\\beta_1$ in Adam (:numref:`sec_adam`) from $0.9$ to $0.5$. It decreases the smoothness of the momentum, the exponentially weighted moving average of past gradients, to take care of the rapid changing gradients because the generator and the discriminator fight with each other. Besides, the random generated noise `Z`, is a 4-D tensor and we are using GPU to accelerate the computation."},{"cell_type":"code","execution_count":21,"metadata":{"attributes":{"classes":[],"id":"","n":"20"},"graffitiCellId":"id_yuwepmb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"00EF240EB4824D6185F29716E49020E6","collapsed":false,"scrolled":false},"outputs":[],"source":"def update_D(X,Z,net_D,net_G,loss,trainer_D):\n    batch_size=X.shape[0]\n    Tensor=torch.cuda.FloatTensor\n    ones=Variable(Tensor(np.ones(batch_size,)),requires_grad=False).view(batch_size,1)\n    zeros = Variable(Tensor(np.zeros(batch_size,)),requires_grad=False).view(batch_size,1)\n    real_Y=net_D(X).view(batch_size,-1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X).view(batch_size,-1)\n    loss_D=(loss(real_Y,ones)+loss(fake_Y,zeros))/2\n    loss_D.backward()\n    trainer_D.step()\n    return float(loss_D.sum())\n\ndef update_G(Z,net_D,net_G,loss,trainer_G):\n    batch_size=Z.shape[0]\n    Tensor=torch.cuda.FloatTensor\n    ones=Variable(Tensor(np.ones((batch_size,))),requires_grad=False).view(batch_size,1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X).view(batch_size,-1)\n    loss_G=loss(fake_Y,ones)\n    loss_G.backward()\n    trainer_G.step()\n    return float(loss_G.sum())\n\n\ndef train(net_D,net_G,data_iter,num_epochs,lr,latent_dim):\n    loss=nn.BCELoss()\n    Tensor=torch.cuda.FloatTensor\n    trainer_D=torch.optim.Adam(net_D.parameters(),lr=lr,betas=(0.5,0.999))\n    trainer_G=torch.optim.Adam(net_G.parameters(),lr=lr,betas=(0.5,0.999))\n    plt.figure(figsize=(7,4))\n    d_loss_point=[]\n    g_loss_point=[]\n    d_loss=0\n    g_loss=0\n    for epoch in range(1,num_epochs+1):\n        d_loss_sum=0\n        g_loss_sum=0\n        batch=0\n        for X in data_iter:\n            X=X[:][0]\n            batch+=1\n            X=Variable(X.type(Tensor))\n            batch_size=X.shape[0]\n            Z=Variable(Tensor(np.random.normal(0,1,(batch_size,latent_dim,1,1))))\n\n            trainer_D.zero_grad()\n            d_loss = update_D(X, Z, net_D, net_G, loss, trainer_D)\n            d_loss_sum+=d_loss\n            trainer_G.zero_grad()\n            g_loss = update_G(Z, net_D, net_G, loss, trainer_G)\n            g_loss_sum+=g_loss\n\n        d_loss_point.append(d_loss_sum/batch)\n        g_loss_point.append(g_loss_sum/batch)\n        print(\n            \"[Epoch %d/%d]  [D loss: %f] [G loss: %f]\"\n            % (epoch, num_epochs,  d_loss_sum/batch_size,  g_loss_sum/batch_size)\n        )\n\n\n    plt.ylabel('Loss', fontdict={ 'size': 14})\n    plt.xlabel('epoch', fontdict={ 'size': 14})\n    plt.xticks(range(0,num_epochs+1,3))\n    plt.plot(range(1,num_epochs+1),d_loss_point,color='orange',label='discriminator')\n    plt.plot(range(1,num_epochs+1),g_loss_point,color='blue',label='generator')\n    plt.legend()\n    plt.show()\n    print(d_loss,g_loss)\n\n    Z = Variable(Tensor(np.random.normal(0, 1, size=(21, latent_dim, 1, 1))),requires_grad=False)\n    fake_x = generator(Z)\n    fake_x=fake_x.cpu().detach().numpy()\n    plt.figure(figsize=(14,6))\n    for i in range(21):\n        im=np.transpose(fake_x[i])\n        plt.subplot(3,7,i+1)\n        plt.imshow(im)\n    plt.show()"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_4qjnatu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B5C0EEA901304E828203209D9C5BED94","mdEditEnable":false},"source":"Now let's train the model."},{"cell_type":"code","execution_count":22,"metadata":{"attributes":{"classes":[],"id":"","n":"21"},"graffitiCellId":"id_739lwqb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"349B9D241985452689DD2AF72D5002FE","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/PIL/Image.py:953: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n  'to RGBA images')\n","name":"stderr"},{"output_type":"stream","text":"[Epoch 1/50]  [D loss: 1.228765] [G loss: 3.742310]\n[Epoch 2/50]  [D loss: 0.722008] [G loss: 6.762116]\n[Epoch 3/50]  [D loss: 0.697246] [G loss: 2.515974]\n[Epoch 4/50]  [D loss: 0.612037] [G loss: 2.568029]\n[Epoch 5/50]  [D loss: 0.571252] [G loss: 2.922787]\n[Epoch 6/50]  [D loss: 0.515668] [G loss: 3.197422]\n[Epoch 7/50]  [D loss: 0.512401] [G loss: 3.457348]\n[Epoch 8/50]  [D loss: 0.522576] [G loss: 3.382301]\n[Epoch 9/50]  [D loss: 0.470414] [G loss: 3.566750]\n[Epoch 10/50]  [D loss: 0.428177] [G loss: 3.866930]\n[Epoch 11/50]  [D loss: 0.410874] [G loss: 3.820086]\n[Epoch 12/50]  [D loss: 0.483233] [G loss: 3.443334]\n[Epoch 13/50]  [D loss: 0.415212] [G loss: 3.797699]\n[Epoch 14/50]  [D loss: 0.406897] [G loss: 3.848611]\n[Epoch 15/50]  [D loss: 0.373717] [G loss: 3.959384]\n[Epoch 16/50]  [D loss: 0.365346] [G loss: 4.267612]\n[Epoch 17/50]  [D loss: 0.267436] [G loss: 5.068651]\n[Epoch 18/50]  [D loss: 0.306835] [G loss: 4.442808]\n[Epoch 19/50]  [D loss: 0.073110] [G loss: 7.539761]\n[Epoch 20/50]  [D loss: 0.000746] [G loss: 51.761187]\n[Epoch 21/50]  [D loss: 0.000054] [G loss: 51.309402]\n[Epoch 22/50]  [D loss: 0.000032] [G loss: 51.108725]\n[Epoch 23/50]  [D loss: 0.000022] [G loss: 50.978125]\n[Epoch 24/50]  [D loss: 0.000016] [G loss: 50.876173]\n[Epoch 25/50]  [D loss: 0.000013] [G loss: 50.783340]\n[Epoch 26/50]  [D loss: 0.000011] [G loss: 50.722527]\n[Epoch 27/50]  [D loss: 0.000009] [G loss: 50.661788]\n[Epoch 28/50]  [D loss: 0.000008] [G loss: 50.596824]\n[Epoch 29/50]  [D loss: 0.000006] [G loss: 50.557074]\n[Epoch 30/50]  [D loss: 0.000006] [G loss: 50.501055]\n[Epoch 31/50]  [D loss: 0.000005] [G loss: 50.451373]\n[Epoch 32/50]  [D loss: 0.000004] [G loss: 50.408567]\n[Epoch 33/50]  [D loss: 0.000004] [G loss: 50.374566]\n[Epoch 34/50]  [D loss: 0.000003] [G loss: 50.328818]\n[Epoch 35/50]  [D loss: 0.000003] [G loss: 50.290861]\n[Epoch 36/50]  [D loss: 0.000003] [G loss: 50.252186]\n[Epoch 37/50]  [D loss: 0.000003] [G loss: 50.216797]\n[Epoch 38/50]  [D loss: 0.000002] [G loss: 50.183566]\n[Epoch 39/50]  [D loss: 0.000002] [G loss: 50.150752]\n[Epoch 40/50]  [D loss: 0.000002] [G loss: 50.113133]\n[Epoch 41/50]  [D loss: 0.000002] [G loss: 50.084053]\n[Epoch 42/50]  [D loss: 0.000002] [G loss: 50.057200]\n[Epoch 43/50]  [D loss: 0.000001] [G loss: 50.021454]\n[Epoch 44/50]  [D loss: 0.000001] [G loss: 49.987773]\n[Epoch 45/50]  [D loss: 0.000001] [G loss: 49.965497]\n[Epoch 46/50]  [D loss: 0.000001] [G loss: 49.945135]\n[Epoch 47/50]  [D loss: 0.000001] [G loss: 49.911026]\n[Epoch 48/50]  [D loss: 0.000001] [G loss: 49.880250]\n[Epoch 49/50]  [D loss: 0.000001] [G loss: 49.846183]\n[Epoch 50/50]  [D loss: 0.000001] [G loss: 49.819711]\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 504x288 with 1 Axes>","text/html":"<img src=\"data:image/png;base64,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\n\">"},"transient":{}},{"output_type":"stream","text":"5.644474754262774e-07 46.77458572387695\n","name":"stdout"},{"output_type":"stream","text":"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","name":"stderr"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 1008x432 with 21 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/349B9D241985452689DD2AF72D5002FE/q66xnokg5.png\">"},"transient":{}}],"source":"if __name__ == '__main__':\n    lr,latent_dim,num_epochs=0.005,100,50\n    train(discriminator,generator,data_iter,num_epochs,lr,latent_dim)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_p313wu3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5B66A53614054FF18B9F11884A64345A","mdEditEnable":false},"source":"## Summary\n\n* DCGAN architecture has four convolutional layers for the Discriminator and four \"fractionally-strided\" convolutional layers for the Generator.\n* The Discriminator is a 4-layer strided convolutions with batch normalization (except its input layer) and leaky ReLU activations. \n* Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem and helps the gradients flow easier through the architecture.\n\n\n## Exercises\n\n* What will happen if we use standard ReLU activation rather than leaky ReLU?\n* Apply DCGAN on Fashion-MNIST and see which category works well and which does not.\n"},{"metadata":{"id":"A8B357578FD64AF48FBAB14D3800E444","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}